{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 636\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Jane knocked on Susan's door but she did not a...</td>\n",
       "      <td>Jane did not answer.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>The donkey wished a wart on its hind leg would...</td>\n",
       "      <td>The donkey wished a wart on its hind leg would...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>The customer walked into the bank and stabbed ...</td>\n",
       "      <td>The customer was immediately taken to the hosp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Grant worked hard to harvest his beans so he a...</td>\n",
       "      <td>Later, he and Tatyana would shell them and coo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>The donkey wished a wart on its hind leg would...</td>\n",
       "      <td>The donkey wished a wart on its hind leg would...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>When the sponsors of the bill got to the town ...</td>\n",
       "      <td>The opponents were very much in the minority.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sam broke both his ankles and he's walking wit...</td>\n",
       "      <td>The crutches should be better.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>The large ball crashed right through the table...</td>\n",
       "      <td>The table was made of styrofoam.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Paul tried to call George on the phone, but he...</td>\n",
       "      <td>George wasn't successful.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Mama came over and sat down beside Alice. Gent...</td>\n",
       "      <td>Alice stroked her hair and let the child weep.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence0  \\\n",
       "59   Jane knocked on Susan's door but she did not a...   \n",
       "375  The donkey wished a wart on its hind leg would...   \n",
       "95   The customer walked into the bank and stabbed ...   \n",
       "326  Grant worked hard to harvest his beans so he a...   \n",
       "540  The donkey wished a wart on its hind leg would...   \n",
       "390  When the sponsors of the bill got to the town ...   \n",
       "28   Sam broke both his ankles and he's walking wit...   \n",
       "297  The large ball crashed right through the table...   \n",
       "281  Paul tried to call George on the phone, but he...   \n",
       "237  Mama came over and sat down beside Alice. Gent...   \n",
       "\n",
       "                                             sentence1 label  \n",
       "59                                Jane did not answer.     0  \n",
       "375  The donkey wished a wart on its hind leg would...     0  \n",
       "95   The customer was immediately taken to the hosp...     0  \n",
       "326  Later, he and Tatyana would shell them and coo...     1  \n",
       "540  The donkey wished a wart on its hind leg would...     0  \n",
       "390      The opponents were very much in the minority.     0  \n",
       "28                      The crutches should be better.     0  \n",
       "297                   The table was made of styrofoam.     1  \n",
       "281                          George wasn't successful.     0  \n",
       "237     Alice stroked her hair and let the child weep.     0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"data/WNLI/train.tsv\",delimiter='\\t',header=None,names=['sentence0','sentence1','label'])\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 3,647\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>ID1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>sentence0</th>\n",
       "      <th>sentence1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>0</td>\n",
       "      <td>462816</td>\n",
       "      <td>462552</td>\n",
       "      <td>Earlier Friday, Taiwan reported 55 new cases b...</td>\n",
       "      <td>Yesterday, Taiwan reported 65 new cases, its b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "      <td>472952</td>\n",
       "      <td>472535</td>\n",
       "      <td>But Mitsubishi Tokyo Financial (JP:8306: news,...</td>\n",
       "      <td>Sumitomo Mitsui Financial (JP:8316: news, char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3429</th>\n",
       "      <td>1</td>\n",
       "      <td>3001417</td>\n",
       "      <td>3001446</td>\n",
       "      <td>Former U.S. Rep. Frank McCloskey, 64, died thi...</td>\n",
       "      <td>McCloskey died Sunday afternoon in his home af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>1</td>\n",
       "      <td>1380723</td>\n",
       "      <td>1380525</td>\n",
       "      <td>In Virginia, Mr. Kilgore, a Republican, accuse...</td>\n",
       "      <td>The Virginia attorney-general, Jerry Kilgore, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>1</td>\n",
       "      <td>2694682</td>\n",
       "      <td>2694609</td>\n",
       "      <td>You know I have always tried to be honest with...</td>\n",
       "      <td>You know I have always tried to be honest with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0</td>\n",
       "      <td>487684</td>\n",
       "      <td>487726</td>\n",
       "      <td>The euro tagged another record high against th...</td>\n",
       "      <td>The euro ros further into record territory on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>1</td>\n",
       "      <td>3030254</td>\n",
       "      <td>3030287</td>\n",
       "      <td>Intel plans to present a paper on its findings...</td>\n",
       "      <td>The breakthrough technology was presented by I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1</td>\n",
       "      <td>3214303</td>\n",
       "      <td>3214275</td>\n",
       "      <td>Marisa Baldeo stated, however, the authority's...</td>\n",
       "      <td>As of now, they are not supposed to wear anyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0</td>\n",
       "      <td>3225863</td>\n",
       "      <td>3225896</td>\n",
       "      <td>The American Express Corp. has pledged at leas...</td>\n",
       "      <td>The city had requested federal funds, but with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>1</td>\n",
       "      <td>2613357</td>\n",
       "      <td>2613445</td>\n",
       "      <td>The condition is associated with heart disease...</td>\n",
       "      <td>Those with diabetes run the risk of severe com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label      ID1      ID2  \\\n",
       "2653     0   462816   462552   \n",
       "82       0   472952   472535   \n",
       "3429     1  3001417  3001446   \n",
       "2690     1  1380723  1380525   \n",
       "2448     1  2694682  2694609   \n",
       "328      0   487684   487726   \n",
       "1960     1  3030254  3030287   \n",
       "288      1  3214303  3214275   \n",
       "1294     0  3225863  3225896   \n",
       "1623     1  2613357  2613445   \n",
       "\n",
       "                                              sentence0  \\\n",
       "2653  Earlier Friday, Taiwan reported 55 new cases b...   \n",
       "82    But Mitsubishi Tokyo Financial (JP:8306: news,...   \n",
       "3429  Former U.S. Rep. Frank McCloskey, 64, died thi...   \n",
       "2690  In Virginia, Mr. Kilgore, a Republican, accuse...   \n",
       "2448  You know I have always tried to be honest with...   \n",
       "328   The euro tagged another record high against th...   \n",
       "1960  Intel plans to present a paper on its findings...   \n",
       "288   Marisa Baldeo stated, however, the authority's...   \n",
       "1294  The American Express Corp. has pledged at leas...   \n",
       "1623  The condition is associated with heart disease...   \n",
       "\n",
       "                                              sentence1  \n",
       "2653  Yesterday, Taiwan reported 65 new cases, its b...  \n",
       "82    Sumitomo Mitsui Financial (JP:8316: news, char...  \n",
       "3429  McCloskey died Sunday afternoon in his home af...  \n",
       "2690  The Virginia attorney-general, Jerry Kilgore, ...  \n",
       "2448  You know I have always tried to be honest with...  \n",
       "328   The euro ros further into record territory on ...  \n",
       "1960  The breakthrough technology was presented by I...  \n",
       "288   As of now, they are not supposed to wear anyth...  \n",
       "1294  The city had requested federal funds, but with...  \n",
       "1623  Those with diabetes run the risk of severe com...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"data/MRPC/train.tsv\",delimiter='\\t',header=None,names=['label','ID1','ID2','sentence0','sentence1'])\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '1' '1' '0' '0' '0' '0' '1' '1' '0' '1' '0' '0' '0' '0' '0' '1' '0'\n",
      " '1' '0' '0' '0' '0' '1' '0' '0' '0' '1' '0' '0' '1' '1' '1' '1' '1' '0'\n",
      " '1' '1' '1' '1' '1' '0' '1' '1' '1' '0' '0' '0' '1' '0' '0' '1' '1' '0'\n",
      " '0' '1' '1' '0' '0' '0' '1' '1' '1' '1' '0' '1' '0' '1' '1' '0' '1' '1'\n",
      " '0' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' '1' '1' '1'\n",
      " '1' '1' '1' '0' '1' '0' '0' '1' '1' '1' '1' '0' '0' '1' '0' '1' '1' '1'\n",
      " '1' '1' '0' '1' '1' '0' '0' '0' '0' '1' '0' '1' '1' '1' '1' '0' '1' '1'\n",
      " '0' '0' '0' '1' '1' '1' '1' '0' '1' '0' '1' '0' '1' '0' '0' '0' '1' '0'\n",
      " '1' '1' '1' '0' '1' '0' '0' '0' '0' '0' '0' '1' '1' '1' '1' '0' '1' '0'\n",
      " '0' '1' '1' '1' '1' '1' '1' '0' '1' '1' '1' '1' '1' '0' '1' '0' '0' '0'\n",
      " '1' '0' '0' '1' '1' '1' '0' '0' '1' '1' '1' '0' '1' '1' '1' '1' '1' '1'\n",
      " '0' '1' '0' '1' '1' '1' '1' '1' '0' '0' '0' '0' '0' '1' '1' '0' '1' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '1' '0' '1' '0' '1' '1' '1' '0'\n",
      " '1' '1' '0' '0' '0' '1' '0' '0' '0' '1' '0' '1' '1' '0' '1' '1' '1' '1'\n",
      " '0' '1' '0' '0' '0' '0' '1' '0' '0' '1' '0' '0' '0' '0' '1' '0' '1' '1'\n",
      " '0' '1' '0' '1' '1' '1' '0' '0' '1' '1' '1' '0' '1' '1' '0' '1' '0' '1'\n",
      " '1' '1' '1' '0' '1' '1' '1' '1' '0' '1' '1' '1' '1' '1' '1' '0' '1' '1'\n",
      " '1' '0' '0' '0' '0' '1' '0' '1' '1' '1' '1' '1' '1' '1' '0' '0' '0' '0'\n",
      " '0' '1' '1' '0' '0' '1' '1' '1' '0' '1' '1' '0' '1' '0' '1' '0' '1' '0'\n",
      " '1' '0' '0' '1' '0' '1' '1' '1' '1' '0' '0' '1' '0' '1' '0' '1' '0' '1'\n",
      " '0' '0' '0' '0' '0' '0' '0' '1' '0' '1' '0' '1' '1' '1' '0' '0' '0' '1'\n",
      " '1' '0' '1' '0' '0' '1' '0' '0' '1' '0' '0' '1' '0' '1' '1' '0' '0' '0'\n",
      " '1' '0' '0' '1' '0' '1' '1' '0' '0' '0' '1' '1' '1' '0' '0' '0' '1' '1'\n",
      " '1' '0' '0' '0' '1' '1' '0' '1' '0' '1' '0' '0' '0' '1' '0' '0' '1' '0'\n",
      " '0' '0' '1' '0' '0' '1' '1' '0' '0' '0' '1' '1' '0' '1' '1' '0' '0' '1'\n",
      " '0' '0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '1' '1' '1' '0' '0' '1' '1'\n",
      " '1' '1' '0' '1' '0' '1' '0' '0' '0' '0' '1' '1' '1' '1' '1' '0' '1' '1'\n",
      " '0' '0' '1' '1' '0' '1' '1' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '1' '1' '0' '0' '1' '0' '0' '1' '0' '1' '1' '0' '1' '0' '0' '1' '1' '1'\n",
      " '1' '0' '0' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0'\n",
      " '0' '1' '0' '0' '1' '1' '0' '0' '0' '1' '1' '0' '0' '1' '1' '0' '1' '1'\n",
      " '1' '1' '1' '0' '1' '1' '1' '1' '1' '1' '1' '1' '0' '0' '0' '1' '0' '0'\n",
      " '0' '1' '0' '1' '0' '1' '0' '0' '1' '0' '1' '1' '1' '0' '1' '1' '0' '0'\n",
      " '0' '0' '0' '0' '1' '0' '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '1' '1' '1' '0' '1' '0' '0' '1' '0' '1' '1' '0' '0' '1' '1' '0' '1'\n",
      " '0' '1' '0' '1' '1']\n"
     ]
    }
   ],
   "source": [
    "sentences = df.sentence0.values[1:]+df.sentence1.values[1:]\n",
    "\n",
    "labels = df.label.values[1:]\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert_uncased_localpath\",do_lower_case = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  107\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens = True)\n",
    "\n",
    "    max_len = max(max_len , len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ST5/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  I stuck a pin through a carrot. When I pulled the pin out, it had a hole.The carrot had a hole.\n",
      "Token IDs: tensor([  101,  1045,  5881,  1037,  9231,  2083,  1037, 25659,  1012,  2043,\n",
      "         1045,  2766,  1996,  9231,  2041,  1010,  2009,  2018,  1037,  4920,\n",
      "         1012,  1996, 25659,  2018,  1037,  4920,  1012,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "input_ids = []\n",
    "attention_masks =[]\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                                        sent,\n",
    "                                        add_special_tokens = True ,#add 'CLS'\n",
    "                                        max_length = 128, \n",
    "                                        pad_to_max_length = True,   #pad & truncate all sentences to max_length\n",
    "                                        return_attention_mask =True,  #construct attn. masks\n",
    "                                        return_tensors = 'pt',     #return pytorch tensors\n",
    "                                        truncation = True\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = [int(label) for label in labels]\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  571 training samples\n",
      "   64 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset , random_split\n",
    "\n",
    "# combine the training inputs into a TensorDataset\n",
    "dataset = TensorDataset(input_ids,attention_masks,labels)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset , val_dataset =random_split(dataset,[train_size,val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,RandomSampler,SequentialSampler\n",
    "\n",
    "batch_size = 32 \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              sampler= RandomSampler(train_dataset),\n",
    "                              batch_size=batch_size)\n",
    "\n",
    "validation_dataloader = DataLoader(val_dataset,\n",
    "                                   sampler=RandomSampler(val_dataset),\n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Out Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert_uncased_localpath and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification , AdamW,BertConfig\n",
    "model_path = \"bert_uncased_localpath\"\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "config = BertConfig.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, \n",
    "                                                      config=config\n",
    "                                                      )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ST5/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 2\n",
    "PATH = \"./wnli_finetuned/wnli_model.pth\"\n",
    "#PATH = \"./mrpc_finetuned/mrpc_model.pth\"\n",
    "patience = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:03<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 001/002 ] loss = 0.70627, acc = 0.51286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 001/002 ] loss = 0.70909, acc = 0.42188\n",
      "Best model found at epoch 0, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:03<00:00,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 002/002 ] loss = 0.69222, acc = 0.55279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 20.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 002/002 ] loss = 0.71512, acc = 0.37500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def train() :\n",
    "    stale = 0\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            input_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            model.zero_grad()\n",
    "            result = model(input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=input_mask, \n",
    "                       labels=labels,\n",
    "                       return_dict=True)\n",
    "            loss = result.loss\n",
    "            logits = result.logits\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "            acc = flat_accuracy(logits , label_ids)\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "\n",
    "       \n",
    "        for batch in tqdm(validation_dataloader):\n",
    "            input_ids = batch[0].to(device)\n",
    "            input_mask = batch[1].to(device)\n",
    "            labels = batch[2].to(device) \n",
    "            with torch.no_grad():\n",
    "                result = model(input_ids,\n",
    "                                token_type_ids=None, \n",
    "                                attention_mask=input_mask,\n",
    "                                labels=labels,\n",
    "                                return_dict=True)\n",
    "                loss = result.loss\n",
    "                logits = result.logits\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = labels.to('cpu').numpy()\n",
    "                acc = flat_accuracy(logits , label_ids)\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "                \n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "        print(f\"[ Valid | {epoch + 1:03d}/{epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "        if valid_acc > best_acc:\n",
    "            print(f\"Best model found at epoch {epoch}, saving model\")\n",
    "            torch.save(model.state_dict(),PATH)\n",
    "            best_acc =valid_acc\n",
    "            stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale > patience:\n",
    "                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n",
    "                break\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance On Test(dev.dsv) Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ST5/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/WNLI/dev.tsv\",delimiter='\\t',header=None,names=['sentence0','sentence1','label'])\n",
    "\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "\n",
    "sentences = df.sentence0.values[1:]+df.sentence1.values[1:]\n",
    "\n",
    "labels = df.label.values[1:]\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                                        sent,\n",
    "                                        add_special_tokens = True ,#add 'CLS'\n",
    "                                        max_length = 128, \n",
    "                                        pad_to_max_length = True,   #pad & truncate all sentences to max_length\n",
    "                                        return_attention_mask =True,  #construct attn. masks\n",
    "                                        return_tensors = 'pt',     #return pytorch tensors\n",
    "                                        truncation = True\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = [int(label) for label in labels]\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16 \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test sentences: 398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ST5/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2606: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"data/MRPC/dev.tsv\",delimiter='\\t',header=None,names=['label','ID1','ID2','sentence0','sentence1'])\n",
    "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "sentences = df.sentence0.values[1:]+df.sentence1.values[1:]\n",
    "\n",
    "labels = df.label.values[1:]\n",
    "\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                                        sent,\n",
    "                                        add_special_tokens = True ,#add 'CLS'\n",
    "                                        max_length = 512, \n",
    "                                        pad_to_max_length = True,   #pad & truncate all sentences to max_length\n",
    "                                        return_attention_mask =True,  #construct attn. masks\n",
    "                                        return_tensors = 'pt',     #return pytorch tensors\n",
    "                                        truncation = True\n",
    "    )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = [int(label) for label in labels]\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Set the batch size.  \n",
    "batch_size = 16 \n",
    "\n",
    "# Create the DataLoader.\n",
    "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert_uncased_localpath and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5/5 [00:00<00:00, 39.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before finetune acc = 0.47321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 38.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After finetune acc = 0.59643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model =  BertForSequenceClassification.from_pretrained(model_path, config=config)\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "prediction_accs = []\n",
    "for batch in tqdm(prediction_dataloader):\n",
    "    input_ids = batch[0].to(device)\n",
    "    input_mask = batch[1].to(device)\n",
    "    labels = batch[2].to(device) \n",
    "    with torch.no_grad():\n",
    "        result = model(input_ids,\n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=input_mask,\n",
    "                        return_dict=True)\n",
    "        logits = result.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        acc = flat_accuracy(logits , label_ids)\n",
    "    prediction_accs.append(acc)\n",
    "        \n",
    "prediction_acc = sum(prediction_accs) / len(prediction_accs)\n",
    "\n",
    "print(f\"Before finetune acc = {prediction_acc:.5f}\")\n",
    "\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "prediction_accs = []\n",
    "for batch in tqdm(prediction_dataloader):\n",
    "    input_ids = batch[0].to(device)\n",
    "    input_mask = batch[1].to(device)\n",
    "    labels = batch[2].to(device) \n",
    "    with torch.no_grad():\n",
    "        result = model(input_ids,\n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=input_mask,\n",
    "                        return_dict=True)\n",
    "        logits = result.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        acc = flat_accuracy(logits , label_ids)\n",
    "    prediction_accs.append(acc)\n",
    "        \n",
    "prediction_acc = sum(prediction_accs) / len(prediction_accs)\n",
    "\n",
    "print(f\"After finetune acc = {prediction_acc:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
