{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pytreebank\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import csv\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigTrans(object):\n",
    "    def __init__(self):\n",
    "        self.dropout = 0.11067975661405677                   \n",
    "        self.num_classes = 5                # 类别数\n",
    "        self.warm_epochs = 20\n",
    "        self.num_epochs = 100              # epoch数\n",
    "        self.batch_size = 128         # mini-batch大小              \n",
    "        self.learning_rate =0.0011604442299768141           # 学习率\n",
    "        self.weight_decay = 1e-4\n",
    "        self.patience = 40\n",
    "        self.num_layers = 1\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.PATH = './sst5_net.pth'\n",
    "        self.best_acc = 0\n",
    "        self.stale=0\n",
    "        self.hidden_dim=128\n",
    "        self.token_label_dim = 256\n",
    "config = ConfigTrans()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理（函数解释）\n",
    "- get_english_tokenizer():这个函数返回一个英文文本分词器，使用PyTorch的get_tokenizer函数以\"basic_english\"配置创建。它将在后续的文本数据预处理中使用。\n",
    "- data_transform():这个函数用于对数据集进行预处理。它接受一个数据集（如训练集、验证集或测试集），并对每个样本执行以下操作：\n",
    "将文本转为小写。\n",
    "将文本拆分成标签和句子，然后提取句子的标签和单词。\n",
    "使用英文分词器分词句子。\n",
    "构建数据、标签以及每个标记的标签列表。\n",
    "这样操作是为了方便之后给LSTM模型输入\n",
    "- build_vocab():用于构建词汇表。它接受数据和一个最小频率参数，然后使用torchtext库的build_vocab_from_iterator函数构建词汇表。词汇表中包括特殊令牌（如\"<unk>\"），并设置默认索引为\"<unk>\"，以处理未知单词。\n",
    "- load_glove():用于加载与训练的Glove词嵌入模型，从指定路径加载包含词嵌入的文本文件。\n",
    "- embedding_transform():将词汇表中的每个单词映射为词向量,未知单词会生成随机嵌入。\n",
    "- sentence_to_idx():将文本数据和标签转化为模型可用的索引。它将文本转化为单词索引序列，用<pad>令牌进行填充，并创建适合训练的数据和标签张量。\n",
    "- add_pad_embedding()：用于添加一个随机初始化的<pad>令牌的词嵌入到词嵌入矩阵中。\n",
    "- MyDataset():用于包装数据。\n",
    "- get_dataloader_vocab_embedding()：这是主要的数据加载和预处理流程。它首先加载数据集，然后通过data_transform函数对数据进行预处理。接着，它构建词汇表（如果词汇表文件不存在）并加载预训练词嵌入模型。最后，它将数据转化为索引表示，构建数据加载器，并返回词汇表、词嵌入、训练数据加载器、验证数据加载器和测试数据加载器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_english_tokenizer():\n",
    "    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def data_transform(dataset):\n",
    "\n",
    "    tokenizer = get_english_tokenizer()\n",
    "    labels = []\n",
    "    data = []\n",
    "    label_for_each_token = []\n",
    "    for sample in dataset:\n",
    "        sample.lowercase()\n",
    "        sample = sample.to_labeled_lines()\n",
    "        token_label = [i[0] for i in sample]\n",
    "        token = [i[1] for i in sample]\n",
    "\n",
    "        sentence = tokenizer(sample[0][1])\n",
    "        labels.append(sample[0][0])\n",
    "\n",
    "        data_temp = []\n",
    "        label_temp = []\n",
    "        for i in range(len(token)):\n",
    "            if token[i] in sentence:\n",
    "                data_temp.append(token[i])\n",
    "                label_temp.append(token_label[i])\n",
    "        data.append(data_temp)\n",
    "        label_for_each_token.append(label_temp)\n",
    "\n",
    "    return data, labels, label_for_each_token\n",
    "\n",
    "\n",
    "def build_vocab(data, min_freq=1):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        data,\n",
    "        specials=[\"<unk>\"],\n",
    "        min_freq=min_freq\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def load_glove(path=\"glove.6B.300d.txt\"):\n",
    "\n",
    "    words = pd.read_table(path, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    embedding = words.values\n",
    "    words = words.index.to_numpy()\n",
    "\n",
    "    print(\"The embedding size is {}, and the number of words is {}\".format(embedding.shape[1], embedding.shape[0]))\n",
    "    return words, embedding\n",
    "\n",
    "\n",
    "def embedding_transform(vocab, words, embedding):\n",
    "    word_to_idx = vocab.get_stoi()\n",
    "    num_words = vocab.__len__()\n",
    "    embedding_size = embedding.shape[1]\n",
    "    transformed_embedding = np.zeros((num_words, embedding_size))\n",
    "\n",
    "    # 创建一个字典来存储未知单词的嵌入\n",
    "    unknown_word_embeddings = {}\n",
    "\n",
    "    for token, idx in word_to_idx.items():\n",
    "        if token in words:\n",
    "            token_id = np.argwhere(words == token).item()\n",
    "            transformed_embedding[idx,] = embedding[token_id,]\n",
    "        else:\n",
    "            if token not in unknown_word_embeddings:\n",
    "                # 为未知单词生成随机嵌入\n",
    "                unknown_word_embeddings[token] = np.random.rand(embedding_size)\n",
    "            transformed_embedding[idx,] = unknown_word_embeddings[token]\n",
    "\n",
    "    return transformed_embedding\n",
    "\n",
    "\n",
    "\n",
    "def sentence_to_idx(data, vocab, token_label, MAX_SEQUENCE_LENGTH=25):\n",
    "\n",
    "    pad_id = vocab.__len__() - 1\n",
    "    data_size = len(data)\n",
    "    num_class = 5\n",
    "    transformed_data = np.zeros((data_size, MAX_SEQUENCE_LENGTH))\n",
    "    transformed_label = np.zeros((data_size, MAX_SEQUENCE_LENGTH, num_class))\n",
    "\n",
    "    for i in range(data_size):\n",
    "        sentence = data[i]\n",
    "        labels = token_label[i]\n",
    "        len_sen = len(sentence)\n",
    "        sentence = sentence[:MAX_SEQUENCE_LENGTH]\n",
    "        labels = labels[:MAX_SEQUENCE_LENGTH]\n",
    "        word_ids = [vocab[i] for i in sentence]\n",
    "\n",
    "        if len_sen < MAX_SEQUENCE_LENGTH:\n",
    "            padding_length = MAX_SEQUENCE_LENGTH - len_sen\n",
    "            word_ids.extend([pad_id] * padding_length)\n",
    "\n",
    "        transformed_data[i,] = np.array(word_ids, dtype=np.int64)\n",
    "        for j in range(len(labels)):\n",
    "            transformed_label[i, j, labels[j] - 1] = 1\n",
    "\n",
    "    transformed_data = torch.tensor(transformed_data, dtype=torch.long)\n",
    "    transformed_label = torch.tensor(transformed_label, dtype=torch.float)\n",
    "\n",
    "    return transformed_data, transformed_label\n",
    "\n",
    "\n",
    "def add_pad_embedding(embedding, seed=21):\n",
    "   \n",
    "    embedding_dim = embedding.shape[1]\n",
    "    np.random.seed(seed)\n",
    "    pad_embedding = np.random.rand(1, embedding_dim)\n",
    "\n",
    "    return np.concatenate((embedding, pad_embedding), axis=0)\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, label_each_token):\n",
    "        self.data = x\n",
    "        self.label = y\n",
    "        self.label_each_token = label_each_token\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label_each_token[idx], self.label[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def search_file(file_name, search_path, pathsep = os.pathsep):\n",
    "\n",
    "    for path in search_path.split(pathsep):\n",
    "        candidate = os.path.join(path, file_name)\n",
    "        if os.path.isfile(candidate):\n",
    "            return os.path.abspath(candidate)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_dataloader_vocab_embedding(batch_size=config.batch_size, dataset_path='.data/sst/trees', embedding_path='.vector_cache/glove.6B.300d.txt',\n",
    "                                   MAX_SEQUENCE_LENGTH=25):\n",
    "    # Load dataset\n",
    "    dataset = pytreebank.load_sst(dataset_path)\n",
    "    train = dataset['train']\n",
    "    val = dataset['dev']\n",
    "    test = dataset['test']\n",
    "\n",
    "    # data transformation\n",
    "    train_data, train_labels, train_token_labels = data_transform(train)\n",
    "    val_data, val_labels, val_token_labels = data_transform(val)\n",
    "    test_data, test_labels, test_token_labels = data_transform(test)\n",
    "\n",
    "    # label transformation\n",
    "    train_labels = torch.LongTensor(train_labels)\n",
    "    val_labels = torch.LongTensor(val_labels)\n",
    "    test_labels = torch.LongTensor(test_labels)\n",
    "\n",
    "    if search_file('vocab.pt', dataset_path):\n",
    "        vocab = torch.load(os.path.join(dataset_path, \"vocab.pt\"))\n",
    "        print(\"Load vocab directly.\")\n",
    "    else:\n",
    "        # build vocabulary\n",
    "        vocab = build_vocab(train_data)\n",
    "        # vocab add <pad>\n",
    "        vocab.insert_token(\"<PAD>\", vocab.__len__())\n",
    "        print(\"Build vocab.\")\n",
    "    print(\"The vocabulary contains {} words.\".format(vocab.__len__()))\n",
    "\n",
    "    if search_file(\"transformed_embedding.pt\", dataset_path):\n",
    "        transformed_embedding = torch.load(os.path.join(dataset_path, \"transformed_embedding.pt\"))\n",
    "        print(\"Load transformed embeddings directly.\")\n",
    "    else:\n",
    "        # load pre-trained embedding\n",
    "        words, embedding = load_glove(embedding_path)\n",
    "\n",
    "        # get the embedding of our vocabulary\n",
    "        transformed_embedding = embedding_transform(vocab, words, embedding)\n",
    "        # add <pad> embedding\n",
    "        transformed_embedding = add_pad_embedding(transformed_embedding)\n",
    "        transformed_embedding = torch.tensor(transformed_embedding, dtype=torch.float)\n",
    "\n",
    "    # transform the words into ids\n",
    "    train_data, train_token_labels = sentence_to_idx(train_data, vocab, train_token_labels, MAX_SEQUENCE_LENGTH)\n",
    "    val_data, val_token_labels = sentence_to_idx(val_data, vocab, val_token_labels, MAX_SEQUENCE_LENGTH)\n",
    "    test_data, test_token_labels = sentence_to_idx(test_data, vocab, test_token_labels, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    # build datasets\n",
    "    train_dataset = MyDataset(train_data, train_labels, train_token_labels)\n",
    "    val_dataset = MyDataset(val_data, val_labels, val_token_labels)\n",
    "    test_dataset = MyDataset(test_data, test_labels, test_token_labels)\n",
    "\n",
    "    # build dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    return vocab, transformed_embedding, train_loader, val_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load vocab directly.\n",
      "The vocabulary contains 16517 words.\n",
      "Load transformed embeddings directly.\n"
     ]
    }
   ],
   "source": [
    "vocab,transformed_embedding,trainloader,validloader,testloader = get_dataloader_vocab_embedding()\n",
    "classes = ('positive', 'negative', 'neutral', 'very positive', 'very negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct my LSTM、RNN、Transformer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionEncoding 便于在Transformer中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # sin and cos position encoding\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三个模型，分别是LSTM，RNN，Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便，dropout、embedding、attention的影响都在LSTM模型上进行测试。\n",
    "- embedding：在class LSTM_net中，if embedding 来判断是否使用预训练的词向量嵌入，若embedding==None，就使用随机初始化的方法。\n",
    "- dropout：实验设计比较简单，设置dropout的不同值来测试即可。\n",
    "- attention：class Attention中定义，分别在softmax前加和不加attention来测试影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        energy = self.projection(encoder_outputs)\n",
    "        weights = F.softmax(energy.squeeze(-1), dim=1)\n",
    "        outputs = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n",
    "        return outputs\n",
    "\n",
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, embedding, embed_size, vocab_size, hidden_dim, token_label_dim, num_layers, dropout=0):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        if embedding is not None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.embedding.weight = nn.Parameter(embedding)\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            self.pretrain = 1\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, max_norm=1)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "            self.pretrain = 0\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_label_dim = token_label_dim\n",
    "        self.dropout = dropout\n",
    "        self.linear = nn.Linear(5, token_label_dim)\n",
    "        if num_layers == 1:\n",
    "            self.lstm = nn.LSTM(embed_size + token_label_dim,\n",
    "                                hidden_dim,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=0)\n",
    "        else:\n",
    "            self.lstm = nn.LSTM(embed_size + token_label_dim,\n",
    "                                hidden_dim,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                                        nn.Linear(hidden_dim, 5),\n",
    "                                        nn.Softmax(dim=1))\n",
    "        self.model_name = \"LSTM\"\n",
    "        # nn.init.orthogonal_(self.lstm.weight_ih_l0)\n",
    "        # nn.init.orthogonal_(self.lstm.weight_hh_l0)\n",
    "        # nn.init.zeros_(self.lstm.bias_ih_l0)\n",
    "        # nn.init.zeros_(self.lstm.bias_hh_l0)\n",
    "\n",
    "    def forward(self, inputs_0, inputs_1):\n",
    "        x1 = self.embedding(inputs_0)\n",
    "        x2 = self.linear(inputs_1)\n",
    "        x = torch.cat((x1, x2), dim=2)\n",
    "        x, _ = self.lstm(x, None)\n",
    "        # x = [batch, seq_len, hidden_size]\n",
    "        x = self.attention(x)\n",
    "        # x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN_net(nn.Module):\n",
    "    def __init__(self, embedding, embed_size, vocab_size, hidden_dim, token_label_dim, num_layers, dropout=0):\n",
    "        super(RNN_net, self).__init__()\n",
    "        if embedding is not None:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.embedding.weight = nn.Parameter(embedding)\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            self.pretrain = 1\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, max_norm=1)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "            self.pretrain = 0\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.token_label_dim = token_label_dim\n",
    "        self.dropout = dropout\n",
    "        self.linear = nn.Linear(5, token_label_dim)\n",
    "        if num_layers == 1:\n",
    "            self.rnn = nn.RNN(embed_size + token_label_dim,\n",
    "                                hidden_dim,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=0)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embed_size + token_label_dim,\n",
    "                                hidden_dim,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first=True,\n",
    "                                dropout=dropout)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                                        nn.Linear(hidden_dim, 5),\n",
    "                                        nn.Softmax(dim=1))\n",
    "        self.model_name = \"RNN\"\n",
    "        # nn.init.orthogonal_(self.lstm.weight_ih_l0)\n",
    "        # nn.init.orthogonal_(self.lstm.weight_hh_l0)\n",
    "        # nn.init.zeros_(self.lstm.bias_ih_l0)\n",
    "        # nn.init.zeros_(self.lstm.bias_hh_l0)\n",
    "\n",
    "    def forward(self, inputs_0, inputs_1):\n",
    "        x1 = self.embedding(inputs_0)\n",
    "        x2 = self.linear(inputs_1)\n",
    "        x = torch.cat((x1, x2), dim=2)\n",
    "        x, _ = self.rnn(x, None)\n",
    "        # x = [batch, seq_len, hidden_size]\n",
    "        x = self.attention(x)\n",
    "        # x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer_net(nn.Module):\n",
    "    def __init__(self, embedding, embed_size, vocab_size,num_classes, num_layers=1, dropout=0, max_len=128,activation: str = \"relu\"):\n",
    "        super(Transformer_net, self).__init__()\n",
    "        if embedding is not None:\n",
    "            self.num_class = 5\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "            self.embedding.weight = nn.Parameter(embedding)\n",
    "            self.position_embedding = PositionalEncoding(embed_size, dropout, max_len)\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            self.pretrain = 1\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_size, max_norm=1)\n",
    "            self.embedding.weight.requires_grad = True\n",
    "            self.pretrain = 0\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_size ,nhead=2, dim_feedforward=512, dropout=dropout,activation=activation)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        # 输出层\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                                        nn.Linear(embed_size, num_classes),\n",
    "                                        nn.Softmax(dim=1))\n",
    "        self.model_name=\"Transformer\"\n",
    "\n",
    "    def forward(self, inputs ,lengths):\n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "        hidden_states = self.embedding(inputs)\n",
    "        hidden_states = self.position_embedding(hidden_states)\n",
    "        hidden_states = self.transformer(hidden_states)\n",
    "        hidden_states = hidden_states[0, :, :]\n",
    "        x = self.classifier(hidden_states)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试了一些之前训练的小技巧\n",
    "get_cosine_shedule_with_warmup():调度学习率（带有热身），每隔一段时间会修正学习率，整体上先快后慢。\n",
    "LabelSmoothing():标签平滑化，防止模型过于自信，但由于标签不是onehot格式所以实际效果不佳。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedule\n",
    "def get_cosine_shedule_with_warmup(\n",
    "        optimizer: Optimizer,\n",
    "        num_warmup_steps: int,\n",
    "        num_training_steps: int,\n",
    "        num_cycles: float = 0.5,\n",
    "        last_epoch: int = -1\n",
    "):\n",
    "    def lr_lambda(current_step):\n",
    "        # warmup\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        # decadence\n",
    "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress)))\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        logprobs = F.log_softmax(x, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the networtk and save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器选择的是AdamW，lr，weight_decay都是通过网格或者随机搜索得到的较佳值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "embed_size = transformed_embedding.shape[1]\n",
    "vocab_size = vocab.__len__()\n",
    "hidden_dim = config.hidden_dim\n",
    "num_layers = config.num_layers\n",
    "dropout = config.dropout\n",
    "num_classes = config.num_classes\n",
    "token_label_dim = config.token_label_dim\n",
    "net = Transformer_net(transformed_embedding,embed_size,vocab_size,num_classes,num_layers=num_layers,dropout=dropout)\n",
    "#net = LSTM_net(transformed_embedding, embed_size, vocab_size, hidden_dim, token_label_dim, num_layers, dropout)\n",
    "#net = RNN_net(transformed_embedding, embed_size, vocab_size, hidden_dim, token_label_dim, num_layers, dropout)\n",
    "if(net.model_name==\"Transformer\"):\n",
    "    Trans_flag = True\n",
    "else:\n",
    "    Trans_flag = False\n",
    "print(Trans_flag)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = LabelSmoothing(smoothing = 0.19649094880484694)\n",
    "optimizer =optim.AdamW(net.parameters(), lr=config.learning_rate , weight_decay=config.weight_decay)\n",
    "scheduler = get_cosine_shedule_with_warmup(optimizer, config.warm_epochs, config.num_epochs)\n",
    "PATH = config.PATH\n",
    "device = config.device\n",
    "print(device)\n",
    "model = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参调参使用网格搜索和随机搜索寻找最佳超参数，寻找的范围定义在了hyperparameter_space中，打印超参数和准确率，同时将超参数保存到config.txt文件中，效果较好。以LSTM模型为例，最终寻找到的最佳值为：\n",
    "\n",
    "    ```self.dropout = 0.11067975661405677                   \n",
    "        self.num_classes = 5                # 类别数\n",
    "        self.warm_epochs = 50\n",
    "        self.num_epochs = 1000                # epoch数\n",
    "        self.batch_size = 128         # mini-batch大小              \n",
    "        self.learning_rate =0.0011604442299768141           # 学习率\n",
    "        self.weight_decay = 1e-4\n",
    "        self.patience = 150\n",
    "        self.num_layers = 1\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.PATH = './sst5_net.pth'\n",
    "        self.best_acc = 0\n",
    "        self.stale=0\n",
    "        self.hidden_dim=128\n",
    "        self.token_label_dim = 256```\n",
    "使用最佳搜索后LSTM提升了0.025的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, trainloader, validloader, num_epochs , lr_rate , weight_decay , warm_epochs , smoothing):\n",
    "    \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    criterion = LabelSmoothing(smoothing=smoothing)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr= lr_rate , weight_decay = weight_decay)\n",
    "    scheduler = get_cosine_shedule_with_warmup(optimizer,warm_epochs , num_epochs)\n",
    "    model.to(device)\n",
    "    stale = 0\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch in trainloader:\n",
    "            inputs_0 = batch[0].to(device)\n",
    "            inputs_1 = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_0, inputs_1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "        for batch in validloader:\n",
    "            inputs_0 = batch[0].to(device)\n",
    "            inputs_1 = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                 logits = model(inputs_0,inputs_1)\n",
    "            # We can still compute the loss (but not the gradient).\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "            # Compute the accuracy for current batch.\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "            # Record the loss and accuracy.\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "        \n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "        # Print the information.\n",
    "        if valid_acc> best_acc:\n",
    "            best_acc = valid_acc\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale > 50:\n",
    "                #print(f\"No improvment {50} consecutive epochs, early stopping\")\n",
    "                break\n",
    "\n",
    "    return best_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs_0 = batch[0].to(device)\n",
    "            inputs_1 = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(inputs_0, inputs_1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def train_with_random_search():\n",
    "    # Define the hyperparameter space you want to search\n",
    "    hyperparameter_space = {\n",
    "        #'learning_rate': [0.003, 3e-4, 3e-5],\n",
    "        'num_layers': [1, 2, 3],\n",
    "        'hidden_dim': [64, 128, 256],\n",
    "        #'dropout': [0, 0.1, 0.15 ,0.2],\n",
    "        'token_label_dim':[64,128,256],\n",
    "        'batch_size':[64 ,128, 256],\n",
    "        'weight_decay':[1e-5,1e-4,5e-6],\n",
    "        'warm_epochs':[25,50,100]\n",
    "        # Add more hyperparameters as needed\n",
    "    }\n",
    "\n",
    "    # Number of random hyper\n",
    "    # parameter combinations to try\n",
    "    num_searches = 50\n",
    "\n",
    "    best_acc = 0.0\n",
    "    best_hyperparameters = None\n",
    "    for search in range(num_searches):\n",
    "        # Randomly sample hyperparameters from the space\n",
    "        hyperparameters = {\n",
    "            'learning_rate': random.uniform(1e-5,3e-3),\n",
    "            'num_layers': random.choice(hyperparameter_space['num_layers']),\n",
    "            'hidden_dim': random.choice(hyperparameter_space['hidden_dim']),\n",
    "            'dropout':  random.uniform(0,0.2),\n",
    "            'token_label_dim' : random.choice(hyperparameter_space['token_label_dim']),\n",
    "            'batch_size':random.choice(hyperparameter_space['batch_size']),\n",
    "            'weight_decay':random.choice(hyperparameter_space['weight_decay']),\n",
    "            'warm_epochs':random.choice(hyperparameter_space['warm_epochs']),\n",
    "            'smoothing':  random.uniform(0,0.2)\n",
    "            # Add more hyperparameters as needed\n",
    "        }\n",
    "        vocab,transformed_embedding,trainloader,validloader,testloader = get_dataloader_vocab_embedding(batch_size = hyperparameters['batch_size'])\n",
    "        vocab_size = vocab.__len__()\n",
    "        embed_size = transformed_embedding.shape[1]\n",
    "        model = LSTM_net(transformed_embedding, embed_size, vocab_size, hidden_dim = hyperparameters['hidden_dim'], token_label_dim = hyperparameters['token_label_dim'], num_layers = hyperparameters['num_layers'], dropout=hyperparameters['dropout'])\n",
    "        # Train the model and evaluate it\n",
    "        accuracy = train_and_evaluate_model(model, trainloader, validloader, 500 ,lr_rate=hyperparameters['learning_rate'],weight_decay=hyperparameters['weight_decay'],warm_epochs = hyperparameters['warm_epochs'],smoothing = hyperparameters['smoothing'])\n",
    "\n",
    "        # Track the best model and hyperparameters\n",
    "        if accuracy > best_acc:\n",
    "            best_acc = accuracy\n",
    "            best_hyperparameters = hyperparameters\n",
    "\n",
    "    print(f\"Best hyperparameters: {best_hyperparameters}\")\n",
    "    print(f\"Best accuracy: {best_acc}\")\n",
    "    with open('config.txt', 'w+') as config_file:\n",
    "        for key, value in best_hyperparameters.items():\n",
    "            config_file.write(f\"{key}: {value}\\n\")\n",
    "        config_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "train_with_random_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正式训练模型，通过earlystop（设置patience）来缓解过拟合，每次准确率提升都会更新best_acc，并保存模型参数到本地，如果超过patience次没有提升就会结束训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验记录：每次结果都是五次测试取平均值\n",
    "- 两种embedding 方式（以LSTM模型为例）\n",
    "\n",
    "| Embedding | Accuracy |\n",
    "| ----  | ---- |\n",
    "| No   | 0.371 |\n",
    "| Yes  | 0.447 |\n",
    "\n",
    "- dropout对实验的影响（以LSTM模型为例）\n",
    "\n",
    "| Dropout | Accuracy |\n",
    "| ----  | ---- |\n",
    "| 0   | 0.425 |\n",
    "| 0.1  | 0.439 |\n",
    "| 0.2 | 0.447 |\n",
    "\n",
    "- 分类器前加attention对实验的影响（以LSTM模型为例）\n",
    "\n",
    "| Attention | Accuracy |\n",
    "| ----  | ---- |\n",
    "| No   | 0.415 |\n",
    "| Yes  | 0.426 |\n",
    "\n",
    "- 三种不同模型对实验的影响（epoch = 100）（以各自的最佳参数）\n",
    "\n",
    "| Model_name | Accuracy | Time/s |\n",
    "| ----  | ---- | ---- |\n",
    "| LSTM   | 0.442 | 21.6 |\n",
    "| RNN | 0.492 | 44.3 | \n",
    "| Transformer | 0.439 | 37.9 |\n",
    "\n",
    "可以看到RNN的效果最好但是使用时间较长，LSTM的表现适中，Transformer的表现较差。\n",
    "\n",
    "模型性能上：RNN>LSTM>Transformer\n",
    "\n",
    "收敛速度上：LSTM>Transformer>RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 119.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 001/100 ] loss = 1.62864, acc = 0.15757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 316.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 001/100 ] loss = 1.62678, acc = 0.16981\n",
      "Best model found at epoch 0, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 113.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 002/100 ] loss = 1.58087, acc = 0.27013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 298.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 002/100 ] loss = 1.57814, acc = 0.28379\n",
      "Best model found at epoch 1, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 116.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 003/100 ] loss = 1.56917, acc = 0.28992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 313.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 003/100 ] loss = 1.56504, acc = 0.31297\n",
      "Best model found at epoch 2, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 112.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 004/100 ] loss = 1.55417, acc = 0.32350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 004/100 ] loss = 1.54841, acc = 0.32831\n",
      "Best model found at epoch 3, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 114.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 005/100 ] loss = 1.52670, acc = 0.36155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 005/100 ] loss = 1.53308, acc = 0.34825\n",
      "Best model found at epoch 4, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 114.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 006/100 ] loss = 1.51300, acc = 0.37966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 303.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 006/100 ] loss = 1.54672, acc = 0.33844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 118.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 007/100 ] loss = 1.50593, acc = 0.38736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 312.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 007/100 ] loss = 1.51702, acc = 0.37025\n",
      "Best model found at epoch 6, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 116.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 008/100 ] loss = 1.49568, acc = 0.39646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 008/100 ] loss = 1.51741, acc = 0.36967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 113.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 009/100 ] loss = 1.48181, acc = 0.40878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 297.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 009/100 ] loss = 1.52000, acc = 0.37402\n",
      "Best model found at epoch 8, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 115.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 010/100 ] loss = 1.47083, acc = 0.42086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 338.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 010/100 ] loss = 1.51201, acc = 0.37980\n",
      "Best model found at epoch 9, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 112.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 011/100 ] loss = 1.46828, acc = 0.42149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 305.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 011/100 ] loss = 1.51046, acc = 0.38067\n",
      "Best model found at epoch 10, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 109.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 012/100 ] loss = 1.46686, acc = 0.42782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 310.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 012/100 ] loss = 1.50897, acc = 0.37836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 114.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 013/100 ] loss = 1.45854, acc = 0.43373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 302.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 013/100 ] loss = 1.51857, acc = 0.36390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 123.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 014/100 ] loss = 1.47485, acc = 0.42013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 301.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 014/100 ] loss = 1.51631, acc = 0.37983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 111.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 015/100 ] loss = 1.45772, acc = 0.43322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 316.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 015/100 ] loss = 1.50874, acc = 0.38215\n",
      "Best model found at epoch 14, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 116.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 016/100 ] loss = 1.45229, acc = 0.44030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 313.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 016/100 ] loss = 1.50200, acc = 0.37544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 115.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 017/100 ] loss = 1.44100, acc = 0.45227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 272.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 017/100 ] loss = 1.51670, acc = 0.37344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 115.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 018/100 ] loss = 1.45438, acc = 0.44022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 018/100 ] loss = 1.51406, acc = 0.36533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 113.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 019/100 ] loss = 1.44634, acc = 0.44764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 412.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 019/100 ] loss = 1.51424, acc = 0.36914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 118.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 020/100 ] loss = 1.45634, acc = 0.43777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 308.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 020/100 ] loss = 1.52031, acc = 0.36765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 113.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 021/100 ] loss = 1.46021, acc = 0.43175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 426.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 021/100 ] loss = 1.51078, acc = 0.38326\n",
      "Best model found at epoch 20, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 125.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 022/100 ] loss = 1.46582, acc = 0.42751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 282.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 022/100 ] loss = 1.51126, acc = 0.37516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 121.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 023/100 ] loss = 1.44506, acc = 0.44935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 417.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 023/100 ] loss = 1.52381, acc = 0.36275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 118.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 024/100 ] loss = 1.44552, acc = 0.44912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 310.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 024/100 ] loss = 1.51793, acc = 0.37865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 117.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 025/100 ] loss = 1.44286, acc = 0.44842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 375.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 025/100 ] loss = 1.52278, acc = 0.36679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 122.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 026/100 ] loss = 1.43519, acc = 0.45880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 386.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 026/100 ] loss = 1.49804, acc = 0.38905\n",
      "Best model found at epoch 25, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 116.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 027/100 ] loss = 1.43119, acc = 0.46024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 343.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 027/100 ] loss = 1.50233, acc = 0.38647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 125.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 028/100 ] loss = 1.43839, acc = 0.45546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 028/100 ] loss = 1.51682, acc = 0.37344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 113.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 029/100 ] loss = 1.41978, acc = 0.47380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 315.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 029/100 ] loss = 1.52434, acc = 0.36824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 115.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 030/100 ] loss = 1.41856, acc = 0.47442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 305.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 030/100 ] loss = 1.49497, acc = 0.40354\n",
      "Best model found at epoch 29, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 110.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 031/100 ] loss = 1.40677, acc = 0.48449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 263.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 031/100 ] loss = 1.49173, acc = 0.40064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 117.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 032/100 ] loss = 1.40231, acc = 0.48982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 408.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 032/100 ] loss = 1.49104, acc = 0.40325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 124.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 033/100 ] loss = 1.38711, acc = 0.50917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 319.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 033/100 ] loss = 1.50530, acc = 0.38704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 116.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 034/100 ] loss = 1.38955, acc = 0.50707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 034/100 ] loss = 1.50428, acc = 0.39163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 109.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 035/100 ] loss = 1.39133, acc = 0.50529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 417.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 035/100 ] loss = 1.48094, acc = 0.41799\n",
      "Best model found at epoch 34, saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 111.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 036/100 ] loss = 1.38109, acc = 0.51574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 330.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 036/100 ] loss = 1.49271, acc = 0.40034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 132.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 037/100 ] loss = 1.39631, acc = 0.50012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 416.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 037/100 ] loss = 1.52191, acc = 0.37144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 126.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 038/100 ] loss = 1.40587, acc = 0.49098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 343.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 038/100 ] loss = 1.48353, acc = 0.41538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 113.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 039/100 ] loss = 1.39062, acc = 0.50482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 316.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 039/100 ] loss = 1.51922, acc = 0.37665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 128.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 040/100 ] loss = 1.39005, acc = 0.50742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 321.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 040/100 ] loss = 1.48906, acc = 0.40381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 117.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 041/100 ] loss = 1.39217, acc = 0.50544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 407.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 041/100 ] loss = 1.49265, acc = 0.40525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 117.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 042/100 ] loss = 1.37517, acc = 0.52231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 370.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 042/100 ] loss = 1.51700, acc = 0.37518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 112.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 043/100 ] loss = 1.36896, acc = 0.52775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 311.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 043/100 ] loss = 1.50681, acc = 0.38244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 106.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 044/100 ] loss = 1.37262, acc = 0.52573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 044/100 ] loss = 1.51139, acc = 0.38270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 115.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 045/100 ] loss = 1.36132, acc = 0.53630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 370.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 045/100 ] loss = 1.50535, acc = 0.38330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 111.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 046/100 ] loss = 1.36273, acc = 0.53626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 316.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 046/100 ] loss = 1.50586, acc = 0.38298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 110.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 047/100 ] loss = 1.36142, acc = 0.53638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 347.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 047/100 ] loss = 1.52375, acc = 0.36998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 108.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 048/100 ] loss = 1.35359, acc = 0.54485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 324.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 048/100 ] loss = 1.49623, acc = 0.40064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 106.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 049/100 ] loss = 1.35154, acc = 0.54408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 349.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 049/100 ] loss = 1.49627, acc = 0.39688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 111.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 050/100 ] loss = 1.33775, acc = 0.56114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 303.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 050/100 ] loss = 1.48955, acc = 0.40208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 102.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 051/100 ] loss = 1.33769, acc = 0.56168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 316.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 051/100 ] loss = 1.48944, acc = 0.40379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 106.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 052/100 ] loss = 1.34155, acc = 0.55733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 277.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 052/100 ] loss = 1.50448, acc = 0.39168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 117.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 053/100 ] loss = 1.34019, acc = 0.55752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 314.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 053/100 ] loss = 1.48729, acc = 0.40816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 113.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 054/100 ] loss = 1.32324, acc = 0.57587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 312.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 054/100 ] loss = 1.50624, acc = 0.38415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 112.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 055/100 ] loss = 1.32452, acc = 0.57362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 308.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 055/100 ] loss = 1.49032, acc = 0.40064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:00<00:00, 116.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 056/100 ] loss = 1.31165, acc = 0.58994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 385.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Valid | 056/100 ] loss = 1.50579, acc = 0.38589\n",
      "No improvment 20 consecutive epochs, early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    stale = config.stale\n",
    "    best_acc = config.best_acc  \n",
    "    for epoch in range(config.num_epochs):\n",
    "        # ---------- Training ----------\n",
    "        # Make sure the model is in train mode before training.\n",
    "        model.train()\n",
    "\n",
    "        # These are used to record information in training.\n",
    "        train_loss = []\n",
    "        train_accs = []\n",
    "        for batch in tqdm(trainloader):\n",
    "            inputs_0 = batch[0].to(device)\n",
    "            inputs_1 = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            if(Trans_flag):\n",
    "                logits = model(inputs_0,len(inputs_0))\n",
    "            else:\n",
    "                logits =model(inputs_0,inputs_1)\n",
    "            loss = criterion(logits,labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "            train_loss.append(loss.item())\n",
    "            train_accs.append(acc)\n",
    "        train_loss = sum(train_loss) / len(train_loss)\n",
    "        train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "        # Print the information.\n",
    "        print(f\"[ Train | {epoch + 1:03d}/{config.num_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
    "\n",
    "        # ---------- Validation ----------\n",
    "        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        # These are used to record information in validation.\n",
    "        valid_loss = []\n",
    "        valid_accs = []\n",
    "\n",
    "        # Iterate the validation set by batches.\n",
    "        for batch in tqdm(validloader):\n",
    "            inputs_0 = batch[0].to(device)\n",
    "            inputs_1 = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                 if(Trans_flag):\n",
    "                    logits = model (inputs_0,len(inputs_0))\n",
    "                 else: \n",
    "                    logits = model(inputs_0,inputs_1)\n",
    "            # We can still compute the loss (but not the gradient).\n",
    "            loss = criterion(logits, labels.to(device))\n",
    "            # Compute the accuracy for current batch.\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "            # Record the loss and accuracy.\n",
    "            valid_loss.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "            #break\n",
    "\n",
    "        # The average loss and accuracy for entire validation set is the average of the recorded values.\n",
    "        valid_loss = sum(valid_loss) / len(valid_loss)\n",
    "        valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "\n",
    "        # Print the information.\n",
    "        print(f\"[ Valid | {epoch + 1:03d}/{config.num_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n",
    "\n",
    "\n",
    "        # save models\n",
    "        if valid_acc > best_acc:\n",
    "            print(f\"Best model found at epoch {epoch}, saving model\")\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            best_acc = valid_acc\n",
    "            stale = 0\n",
    "        else:\n",
    "            stale += 1\n",
    "            if stale > config.patience:\n",
    "                print(f\"No improvment {config.patience} consecutive epochs, early stopping\")\n",
    "                break\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained model\n",
    "使用测试集得出准确率，（仅是测试）不代表实验过程中最高准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.41416\n"
     ]
    }
   ],
   "source": [
    "# @title\n",
    "# Measure accuracy for each class\n",
    "net = Transformer_net(transformed_embedding,embed_size,vocab_size,num_classes,num_layers=num_layers,dropout=dropout)\n",
    "#net = LSTM_net(transformed_embedding, embed_size, vocab_size, hidden_dim, token_label_dim, num_layers, dropout)\n",
    "#net = RNN_net(transformed_embedding, embed_size, vocab_size, hidden_dim, token_label_dim, num_layers, dropout)\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "device = config.device\n",
    "model = net.to(device)\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "test_accs = []\n",
    "with torch.no_grad():\n",
    "    for batch in testloader:\n",
    "            inputs_0 = batch[0].to(device)\n",
    "            inputs_1 = batch[1].to(device)\n",
    "            labels = batch[2].to(device)\n",
    "            if(Trans_flag):\n",
    "                logits = model (inputs_0,len(inputs_0))\n",
    "            else:\n",
    "                logits = model(inputs_0,inputs_1)\n",
    "            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "            test_accs.append(acc)\n",
    "    test_acc = sum(test_accs) / len(test_accs)\n",
    "    print(f\"acc = {test_acc:.5f}\")\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在SST-5情感分析中，本次使用使用了LSTM、RNN、Transformer三个模型，测试过程中通过网格和随机搜索寻找了各自模型的最佳超参值，三个模型中RNN的效果最好但是耗时较长，LSTM的综合水平较好，Transformer的效果较差，实验中使用dropout、attention、Glove_embedding均能对实验准确率提高，其中embedding的效果最明显。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
